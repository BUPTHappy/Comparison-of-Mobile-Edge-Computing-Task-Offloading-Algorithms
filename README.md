This research compares the convergence speeds of Q-learning and GA in edge computing task offloading, revealing significant differences in performance. The results show that GA outperforms Q-learning regarding initial convergence speed, especially when the state space is ample, the problem is complex, and \gls{GA} can find a near-optimal solution through a fast global search.
However, Q-learning shows strong adaptability in dynamic environments and can optimize resource allocation and task offloading through continuous learning, thus achieving progressive optimization. Although Q-learning has a slower convergence rate in the initial exploration phase, its flexibility and ability to adapt dynamically make it possible to achieve better performance in later stages. For energy consumption, the global optimization capability of \gls{GA} helps to find better offloading solutions, thus reducing the overall energy consumption. However, the \gls{GA} may introduce additional computational overhead during the iterative computation and evolution of multiple solutions, resulting in increased energy consumption. Therefore, the performance of \gls{GA} in terms of energy consumption depends on the specific implementation and the selected stopping conditions. This study provides valuable insights into
edge computing task offloading, which can help choose an appropriate algorithm to optimize system performance.
